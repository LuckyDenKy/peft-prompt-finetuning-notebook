{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045a7079-1075-4bcd-8e72-2b6ae8c9ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba4504a-520a-4a80-9260-50f63a853793",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloomz-560m\"\n",
    "# model_name = \"bigscience/bloom-1b1\"\n",
    "NUM_VIRTUAL_TOKENS = 4\n",
    "NUM_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d194ce4-d1d4-45c7-907c-a1cbbf53af9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f3419a40f74ab59a4a83e5cf803e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuhz\\.conda\\envs\\lhz\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liuhz\\.cache\\huggingface\\hub\\models--bigscience--bloomz-560m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4ab709bcd44942ad133e7c099cf2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6d77f72c9749249fb42d616f3e5a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9810dfa15f604816b6abe2a53aa5745a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dfe70123af4bf6bed9c1e36f3815bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201be708-02fe-49aa-8a20-7ce0ce8ddaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model,inputs,max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # temperature = 2.0,\n",
    "        # top_P = 0.95,\n",
    "        # do_sample=True,\n",
    "        repetition_penalty=1.5,  # Avoid repetition\n",
    "        early_stopping=True,  # The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e2e45-4c50-44ea-b6e5-6179d8679337",
   "metadata": {},
   "source": [
    "微调之前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a69ed5-b67e-4a51-870b-9ced9fb0f629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I want you to act as a motivational coach.  Don't be afraid of being challenged.\"]\n"
     ]
    }
   ],
   "source": [
    "input_prompt = tokenizer(\"I want you to act as a motivational coach. \",return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model,input_prompt,max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt,skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a5b613-aa65-4207-87b0-cae0dc23a7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are two nice things that should matter to you: the price and quality of your product.']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = tokenizer(\"There are two nice things that should matter to you:\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(foundational_model, input_sentences, max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde6405-5aa9-4e2e-9667-3ed7ccb2bfc3",
   "metadata": {},
   "source": [
    "准备数据集\n",
    "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
    "https://huggingface.co/datasets/Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492ddd21-6c78-48bd-989c-db4073c9daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcfbaaf-e3a2-4b95-80cb-aa6d2367fb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ce48db49ad4e028977e72457e8ba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuhz\\.conda\\envs\\lhz\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liuhz\\.cache\\huggingface\\hub\\datasets--fka--awesome-chatgpt-prompts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025016aa214f44999be8846a6e05e006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompts.csv:   0%|          | 0.00/104k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927c22227faa4078849478a34aa55926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7622853aed43ba84e2aaeb9b9614d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "# Create the Dataset to create prompts\n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]),batched=True)\n",
    "train_sample_prompt = data_prompt[\"train\"].select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf334fe-ae3a-417a-b226-0b4199c9f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['act', 'prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399d18cf-9c4c-4ae2-ae5f-f14ca83fc920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act': ['An Ethereum Developer'], 'prompt': ['Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'], 'input_ids': [[186402, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample_prompt[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8c4bfc-d898-43e6-8a24-0987341d547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentences = load_dataset(\"Abirate/english_quotes\")\n",
    "data_sentences = dataset_sentences.map(lambda samples:tokenizer(samples[\"quote\"]),batched=True)\n",
    "train_sample_sentences = data_sentences[\"train\"].select(range(25))\n",
    "train_sample_sentences = train_sample_sentences.remove_columns([\"author\",\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c6d729-ed64-4490-a3b8-cbd351fa1d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873b0cc-b21e-49e3-9655-49632f5f7809",
   "metadata": {},
   "source": [
    "PEFT配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c5b77d4-0c4f-48f0-b94b-78b4956a1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model,PromptTuningConfig,TaskType,PromptTuningInit\n",
    "\n",
    "generation_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM, # The added virtual tokens are initialized with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, # Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0018b-00e9-46d9-ad69-ead5afdba589",
   "metadata": {},
   "source": [
    "创建2个提示微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b3ee01b-8304-4b1d-ac3f-73942a590370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_prompt = get_peft_model(foundational_model,generation_config)\n",
    "print(peft_model_prompt.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "329080fc-f78c-45d5-9fdf-c443c9607788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_sentences = get_peft_model(foundational_model,generation_config)\n",
    "print(peft_model_sentences.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e5609f-9e56-4936-86a0-a4b50c3862c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "def create_training_arguments(path,learning_rate=0.0035,epochs=6):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        use_cpu=True,  # This is necessary for CPU clusters\n",
    "        auto_find_batch_size=True,  # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=epochs\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46601681-82c5-40d6-a00d-b762d474dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"./\"\n",
    "\n",
    "output_directory_prompt = os.path.join(working_dir,\"peft_outputs_prompt\")\n",
    "output_directory_sentences = os.path.join(working_dir,\"peft_outputs_sentences\")\n",
    "\n",
    "os.makedirs(output_directory_prompt,exist_ok=True)\n",
    "os.makedirs(output_directory_sentences,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ddc1623-7349-4010-80ba-68ac7b7a66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liuhz\\.conda\\envs\\lhz\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args_prompt = create_training_arguments(output_directory_prompt,0.003,NUM_EPOCHS)\n",
    "training_args_sentences = create_training_arguments(output_directory_sentences,0.003,NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bef1fc-c056-4a06-be47-3251b04f56c6",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d24175ca-004e-465a-98a5-d8f3a75d1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,DataCollatorForLanguageModeling\n",
    "\n",
    "def create_trainer(model,training_args,train_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer,mlm=False\n",
    "        ), # mlm=False indicates not to use masked language modeling\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26e5b784-a2eb-4806-adbb-f2620be524c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 05:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42, training_loss=3.4480707077752974, metrics={'train_runtime': 355.1765, 'train_samples_per_second': 0.845, 'train_steps_per_second': 0.118, 'total_flos': 64653968375808.0, 'train_loss': 3.4480707077752974, 'epoch': 6.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练第一个模型\n",
    "trainer_prompt = create_trainer(peft_model_prompt,training_args_prompt,train_sample_prompt)\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36a7c9bb-4ce2-42ab-814a-3cbd15d52c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 01:38, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=4.172862688700358, metrics={'train_runtime': 101.6273, 'train_samples_per_second': 1.476, 'train_steps_per_second': 0.236, 'total_flos': 18779108253696.0, 'train_loss': 4.172862688700358, 'epoch': 6.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练第二个模型\n",
    "trainer_sentences = create_trainer(peft_model_sentences,training_args_sentences,train_sample_sentences)\n",
    "trainer_sentences.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf4705c8-619c-4d43-a980-d6dcb4bd7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_prompt.model.save_pretrained(output_directory_prompt)\n",
    "trainer_sentences.model.save_pretrained(output_directory_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab89909-e9ea-4905-9874-68a272884f7f",
   "metadata": {},
   "source": [
    "推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4f5911e-a73b-424b-b415-9330159cce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_prompt = PeftModel.from_pretrained(\n",
    "    foundational_model,\n",
    "    output_directory_prompt,\n",
    "    #device_map=\"auto\",\n",
    "    is_trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9e2126a-6eb0-408d-a84b-d6b8e022ecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I want you to act as a motivational coach.  Don't be afraid of being rude or mean.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "loaded_model_prompt_outputs = get_outputs(loaded_model_prompt,input_prompt)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs,skuip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0bd1c75-7671-43b2-87a7-0d2bdcb8857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_prompt.load_adapter(output_directory_sentences,adapter_name=\"quotes\")\n",
    "loaded_model_prompt.set_adapter(\"quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa4e7408-b0e2-4299-808f-c6c137bdc0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are two nice things that should matter to you: the weather and your health.']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_sentences_outputs = get_outputs(loaded_model_prompt,input_sentences)\n",
    "print(tokenizer.batch_decode(loaded_model_sentences_outputs,skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d412c5b-28dd-42f4-9837-ec4be936e510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
