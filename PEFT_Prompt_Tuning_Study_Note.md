
# ğŸ“˜ ä½¿ç”¨ PEFT è¿›è¡Œæç¤ºå¾®è°ƒ â€”â€” å­¦ä¹ ç¬”è®°

## âœ… 1. é¡¹ç›®ç›®æ ‡

> åˆ©ç”¨ Hugging Face å’Œ PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰æ¡†æ¶ï¼Œåœ¨ `bloomz-560m` åŸºç¡€ä¸Šè¿›è¡Œæç¤ºå¾®è°ƒï¼ˆPrompt Tuningï¼‰ï¼Œæå‡æ¨¡å‹åœ¨æç¤ºå¼ä»»åŠ¡ä¸Šçš„ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚

---

## ğŸ“¦ 2. ä½¿ç”¨æ¨¡å‹ä¸æ•°æ®é›†

- æ¨¡å‹åŸºåº§ï¼š`bigscience/bloomz-560m`ï¼ˆæ”¯æŒå¤šè¯­è¨€/å¤šä»»åŠ¡ï¼‰
- å¾®è°ƒæ–¹æ³•ï¼šPEFT ä¸­çš„ Prompt Tuning
- æ•°æ®é›†ï¼š
  - [`fka/awesome-chatgpt-prompts`](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)ï¼ˆä»»åŠ¡å¼æŒ‡ä»¤ï¼‰
  - [`Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)ï¼ˆè‡ªç„¶å¥å­ï¼‰

---

## ğŸ”§ 3. æ ¸å¿ƒè®­ç»ƒé…ç½®ä¸ä»£ç åˆ†æ

### ğŸ’¡ æ¨¡å‹åˆå§‹åŒ– + Tokenizer åŠ è½½

```python
model_name = "bigscience/bloomz-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
foundational_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
```

### ğŸ§  è½¯æç¤ºå‚æ•°è®¾ç½®

```python
NUM_VIRTUAL_TOKENS = 4
NUM_EPOCHS = 6
```

- `NUM_VIRTUAL_TOKENS`ï¼šç”¨äºæç¤ºå¼•å¯¼çš„è™šæ‹Ÿ token æ•°é‡
- `NUM_EPOCHS`ï¼šè®­ç»ƒè½®æ¬¡

### ğŸš€ æ¨ç†å‡½æ•°è®¾è®¡

```python
def get_outputs(model, inputs, max_new_tokens=100):
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        repetition_penalty=1.5,
        early_stopping=True,
        eos_token_id=tokenizer.eos_token_id
    )
    return outputs
```

### âœ… æ¨ç†æ ·ä¾‹

```python
input_prompt = tokenizer("I want you to act as a motivational coach.", return_tensors="pt")
outputs = foundational_model.generate(...)  # æ¨ç†æœªå¾®è°ƒæ¨¡å‹æ•ˆæœ
```

---

## ğŸ§ª 4. è®­ç»ƒæµç¨‹ï¼ˆç®€è¦ï¼‰

1. ä½¿ç”¨ `PromptTuningConfig` é…ç½®è™šæ‹Ÿ token å¾®è°ƒæ–¹å¼
2. ä½¿ç”¨ `get_peft_model()` åˆ›å»ºæ¨¡å‹
3. ä½¿ç”¨ Hugging Face `Trainer` è®­ç»ƒ

---

## ğŸ“ 5. é”™è¯¯æ’æŸ¥ä¸ä¿®å¤

### âŒ os.mkdir æŠ¥é”™

```python
# é”™è¯¯å†™æ³•ï¼š
os.mkdir(path, existing=True)

# æ­£ç¡®å†™æ³•ï¼š
os.makedirs(path, exist_ok=True)
```

### âŒ generate ç”¨æ³•æ³¨æ„

- `generate()` æ˜¯æ¨ç†æ–¹æ³•ï¼Œéè®­ç»ƒç”¨
- ä½¿ç”¨ `tokenizer.decode()` è·å–æ–‡æœ¬
- æ§åˆ¶é‡å¤æ€§å¯è°ƒ `repetition_penalty`, `top_p`, `temperature` ç­‰

---

## ğŸ§  6. é¢è¯•é—®é¢˜å¡ç‰‡

| é¢è¯•é—®é¢˜ | å›ç­”å»ºè®® |
|----------|----------|
| ä»€ä¹ˆæ˜¯ Prompt Tuningï¼Ÿ | åªè®­ç»ƒå°‘é‡è™šæ‹Ÿ token çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³• |
| num_virtual_tokens æ˜¯ä»€ä¹ˆï¼Ÿ | ä»£è¡¨å¯å­¦ä¹ çš„æç¤º token æ•°é‡ |
| DataCollatorForLanguageModeling ç”¨é€”ï¼Ÿ | è‡ªåŠ¨å¤„ç† padding å’ŒåŠ¨æ€æ©ç  |
| model.generate() å’Œ forward() åŒºåˆ«ï¼Ÿ | ä¸€ä¸ªç”¨äºæ¨ç†ï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒ |
| ä¸ºä»€ä¹ˆé€‰ bloomz è€Œé GPTï¼Ÿ | bloomz æ”¯æŒå¤šè¯­è¨€æŒ‡ä»¤ï¼Œæ³›åŒ–æ€§å¥½ |
| å¦‚ä½•é¿å…ç”Ÿæˆå†…å®¹é‡å¤ï¼Ÿ | è®¾ç½® `repetition_penalty` å’Œ top_pã€temperature |

---

## ğŸ“ 7. ç®€å†æè¿°æ¨è

> ä½¿ç”¨ Hugging Face Transformers å’Œ PEFT æ¡†æ¶ï¼Œåœ¨ BLOOMZ æ¨¡å‹ä¸Šå®ç° Prompt Tuning å¾®è°ƒã€‚ç‹¬ç«‹å®Œæˆæ•°æ®å¤„ç†ã€æ¨¡å‹é…ç½®ã€è®­ç»ƒä¸æ¨ç†æµç¨‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹å¯¹ä»»åŠ¡å¼æç¤ºçš„ç†è§£èƒ½åŠ›ï¼Œå¹¶è®¾è®¡å¤šè½® prompt å®éªŒåˆ†æå¾®è°ƒæ•ˆæœã€‚

---

## ğŸš€ åç»­å»ºè®®

- æ•´ç†ä¸º Notion / GitHub æ–‡æ¡£
- å°è¯•æ·»åŠ  Gradio æ¨ç† Demo
- æ€»ç»“è®­ç»ƒæ—¥å¿—ä¸å¯¹æ¯”ç»“æœ
- å‡†å¤‡ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼šLoRA å¾®è°ƒ / ChatGLM æŒ‡ä»¤è®­ç»ƒ

## é¡¹ç›®æè¿°
ä½¿ç”¨transformerså’Œpeftåº“ï¼Œå¯¹bloomzæ¨¡å‹åšPrompt Finetuningæ–‡æœ¬ç”Ÿæˆä»»åŠ¡CAUSAL_LMã€‚
### æ•°æ®é›†
ä½¿ç”¨[`fka/awesome-chatgpt-prompts`](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)å’Œ[`Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)ã€‚
### åŸºç¡€æ¨¡å‹
tokenizerå’ŒåŸºç¡€æ¨¡å‹ä½¿ç”¨"bigscience/bloomz-560m"ã€‚
### é…ç½®
ä½¿ç”¨PromptTuningConfigé…ç½®ç›¸å…³å‚æ•°ï¼š
```python
generation_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM, # This type indicates the model will generate text.
    prompt_tuning_init=PromptTuningInit.RANDOM, # The added virtual tokens are initialized with random numbers
    num_virtual_tokens=NUM_VIRTUAL_TOKENS, # Number of virtual tokens to be added and trained.
    tokenizer_name_or_path=model_name
)
```
Trainerä½¿ç”¨DataCollatorForLanguageModelingä½œä¸ºæ•°æ®æ•´ç†å™¨