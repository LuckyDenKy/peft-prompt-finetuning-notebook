import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å¾®è°ƒåçš„æœ¬åœ°è·¯å¾„ï¼‰
model_name = "path/to/your/peft/model"  # æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-560m")
foundational_model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-560m",trust_remote_code=True)
loaded_model_prompt = PeftModel.from_pretrained(
    foundational_model,
    "peft_outputs_prompt",
    device_map="auto",
    is_trainable=False,
)

# æ¨ç†å‡½æ•°
def get_outputs(model,inputs,max_new_tokens=100):
    outputs = model.generate(
        input_ids = inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        # temperature = 2.0,
        # top_P = 0.95,
        # do_sample=True,
        repetition_penalty=1.5,  # Avoid repetition
        early_stopping=True,  # The model can stop before reach the max_length
        eos_token_id=tokenizer.eos_token_id
    )
    return outputs
    
def generate_text(prompt, max_new_tokens=100):
    # å¯¹è¾“å…¥ prompt ç¼–ç 
    inputs = tokenizer(prompt, return_tensors="pt")
    # ä½¿ç”¨å·²æœ‰çš„æ¨ç†å‡½æ•°
    outputs = get_outputs(loaded_model_prompt, inputs, max_new_tokens=max_new_tokens)
    # è§£ç å¹¶è¿”å›ç»“æœ
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# Gradio ç•Œé¢
iface = gr.Interface(
    fn=generate_text,
    inputs=gr.Textbox(lines=4, placeholder="è¾“å…¥ä½ çš„æç¤º..."),
    outputs="text",
    title="ğŸ§  Prompt Tuning Demo",
    description="è¯•è¯•ä½ çš„ prompt ä¼šå¾—åˆ°æ€æ ·çš„æ¨¡å‹è¾“å‡ºï¼Ÿï¼ˆæ¨¡å‹ï¼šPEFT + BLOOMZï¼‰"
)

iface.launch()
